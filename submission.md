E-Commerce Data Pipeline Project
ğŸ‘¤ Student Information

Name: Lahari Sri Kotipalli

Roll Number: 23MH1A05I0

Email: laharisrikotipalli07@gmail.com

Submission Date: 2025-12-25

ğŸ”— GitHub Repository

Repository Name: ecommerce-data-pipeline-23MH1A05I0

Repository URL:

https://github.com/Laharisrikotipalli/ecommerce-data-pipeline-23MH1A05I0


Repository Visibility: Public

Release Tag: v1.0

âœ… Project Completion Status (7 Phases)
Phase	Description	Status
Phase 1	Data Generation (CSV creation)	âœ… Completed
Phase 2	Data Ingestion (Staging schema)	âœ… Completed
Phase 3	Data Transformation (Production schema)	âœ… Completed
Phase 4	Data Quality Checks & Reporting	âœ… Completed
Phase 5	Warehouse Modeling (Star Schema)	âœ… Completed
Phase 6	Orchestration & Scheduling	âœ… Completed
Phase 7	BI Dashboard & Analytics	âœ… Completed
ğŸ§± Architecture Overview

Schema Design:

staging â†’ raw ingested data

production â†’ cleaned & normalized data

warehouse â†’ star schema (fact & dimensions)

Fact Table: warehouse.fact_sales

Dimensions: dim_customers, dim_products, dim_date, dim_payment_method

Architecture Documentation:

architecture.md

âš™ï¸ Technology Stack

Language: Python 3.9

Database: PostgreSQL 14

Containerization: Docker & Docker Compose

CI/CD: GitHub Actions

Testing: Pytest + pytest-cov

BI Tool: Power BI Desktop / Tableau Public

ğŸ³ Docker Setup

Files Included:

Dockerfile

docker-compose.yml

docker/README.md

Services:

PostgreSQL

Data Pipeline Application

Verification:

Containers start successfully

Database persists using Docker volumes

Pipeline runs end-to-end inside Docker

ğŸ” CI/CD Pipeline

Workflow File: .github/workflows/ci.yml

Triggers: Push & Pull Request

Pipeline Steps:

Install dependencies

Start PostgreSQL service

Create schemas

Run unit tests

Enforce test coverage

Test Coverage: >80% (PASS)

ğŸ§ª Testing Summary

Total Tests: 16

Test Types:

Unit tests

Schema validation tests

Quality checks

Orchestrator tests

Coverage: 81%+

ğŸ“Š BI Dashboard
Option Used:

âœ… Power BI Desktop

Artifacts Provided:

.pbix file

4 dashboard screenshots

Metadata JSON

Dashboards Include:

Executive Overview

Product Performance

Customer Segmentation

Revenue & Trends

Dashboard Guide:

dashboard_guide.md

ğŸ“ Data Artifacts Included
Raw Data (CSV)

customers.csv

products.csv

transactions.csv

transaction_items.csv

Pipeline Reports (JSON)

ingestion_summary.json

quality_report.json

transformation_summary.json

pipeline_execution_report.json

monitoring_report.json

Analytical Outputs

10 SQL query result CSV files

â–¶ï¸ Running Instructions
Local Execution
pip install -r requirements.txt
python scripts/pipeline_orchestrator.py

Docker Execution
docker-compose up --build

Run Tests
pytest tests/ -v

ğŸ“ˆ Project Statistics

Total Python Lines of Code: ~XXXX

SQL Scripts: XX

Test Files: XX

Records Processed per Run: ~XXXX

Test Coverage: 81%+

âš ï¸ Challenges Faced & Solutions
Challenge 1: Database availability in CI

Solution: Used GitHub Actions PostgreSQL service with health checks

Challenge 2: Long-running scheduler in CI

Solution: Disabled scheduler loop during CI runs

Challenge 3: Import path issues

Solution: Proper package structure + PYTHONPATH configuration

Challenge 4: Test coverage enforcement

Solution: Focused coverage on core orchestration logic

ğŸ” Verification Checklist

 Repository is public

 All required files committed

 .gitignore excludes secrets & logs

 Docker Compose works correctly

 CI pipeline runs successfully

 Tests pass with >80% coverage

 Documentation complete

 Dashboard artifacts included

ğŸ Final Notes

This project implements a production-grade data engineering pipeline with:

Clean architecture

Automated testing

Containerized deployment

CI/CD integration

BI-ready warehouse modeling

The solution is fully reproducible, well-documented, and operationally ready.