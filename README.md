# E-Commerce Data Analytics Pipeline
## Project Overview

This project implements an end-to-end e-commerce data analytics pipeline that generates synthetic data, ingests it into PostgreSQL, processes it through multiple schema layers, and visualizes insights using Power BI / Tableau.

The pipeline follows modern data engineering best practices including staging, production, warehouse modeling, orchestration, testing, and containerization.
***Architecture Layers***
***Data Generation***
***Data Ingestion***
***Staging Schema***
***Production Schema***
***Warehouse Schema***
***Analytics Layer***
***BI Visualization Layer***
***Orchestration & Monitoring***

---

## ğŸ—ï¸ Architecture Overview

![Architecture Overview](docs/images/architecture_overview.png)

This diagram shows the high-level data flow from synthetic data generation to BI visualization.

---
## ğŸ“‚ Project Structure
```
ecommerce-data-pipeline/
â”‚
â”œâ”€â”€ config/
â”‚ â””â”€â”€ config.yaml
â”‚
â”œâ”€â”€ dashboards/
â”‚ â”œâ”€â”€ powerbi/
â”‚ â”‚ â”œâ”€â”€ ecommerce_analytics.pbix
â”‚ â”‚ â””â”€â”€ ecommerce_analytics.pbit
â”‚ â””â”€â”€ screenshots/
â”‚ â”œâ”€â”€ page1_executive.png
â”‚ â”œâ”€â”€ page2_product.png
â”‚ â”œâ”€â”€ page3_customer.png
â”‚ â””â”€â”€ page4_geographic.png
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/
â”‚ â”‚ â”œâ”€â”€ customers.csv
â”‚ â”‚ â”œâ”€â”€ products.csv
â”‚ â”‚ â”œâ”€â”€ transactions.csv
â”‚ â”‚ â””â”€â”€ transaction_items.csv
â”‚ â”‚
â”‚ â”œâ”€â”€ staging/
â”‚ â”‚ â””â”€â”€ ingestion_summary.json
â”‚ â”‚
â”‚ â””â”€â”€ processed/
â”‚ â”œâ”€â”€ analytics/
â”‚ â”‚ â”œâ”€â”€ query1_top_products.csv
â”‚ â”‚ â”œâ”€â”€ query2_monthly_sales.csv
â”‚ â”‚ â”œâ”€â”€ query3_customer_segments.csv
â”‚ â”‚ â”œâ”€â”€ query4_category_sales.csv
â”‚ â”‚ â”œâ”€â”€ query5_payment_methods.csv
â”‚ â”‚ â”œâ”€â”€ query6_geography.csv
â”‚ â”‚ â”œâ”€â”€ query7_customers.csv
â”‚ â”‚ â”œâ”€â”€ query8_products.csv
â”‚ â”‚ â”œâ”€â”€ query9_daily_sales.csv
â”‚ â”‚ â””â”€â”€ query10_discounts.csv
â”‚ â”‚
â”‚ â”œâ”€â”€ monitoring_reports.json
â”‚ â”œâ”€â”€ pipeline_execution.json
â”‚ â”œâ”€â”€ transformation_summary.json
â”‚ â””â”€â”€ quality_reports/
â”‚ â””â”€â”€ quality_report.json
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ data_generation/
â”‚ â”‚ â””â”€â”€ generate_data.py
â”‚ â”‚
â”‚ â”œâ”€â”€ ingestion/
â”‚ â”‚ â””â”€â”€ ingest_to_staging.py
â”‚ â”‚
â”‚ â”œâ”€â”€ transformation/
â”‚ â”‚ â”œâ”€â”€ cleanup_old_data.py
â”‚ â”‚ â”œâ”€â”€ load_dim_date.py
â”‚ â”‚ â”œâ”€â”€ load_warehouse.py
â”‚ â”‚ â””â”€â”€ staging_to_production.py
â”‚ â”‚
â”‚ â”œâ”€â”€ database/
â”‚ â”‚ â””â”€â”€ warehouse_setup.sql
â”‚ â”‚
â”‚ â”œâ”€â”€ monitoring/
â”‚ â”‚ â””â”€â”€ pipeline_monitor.py
â”‚ â”‚
â”‚ â”œâ”€â”€ quality_checks/
â”‚ â”‚ â””â”€â”€ validate_data.py
â”‚ â”‚
â”‚ â”œâ”€â”€ pipeline_orchestrator.py
â”‚ â””â”€â”€ scheduler.py
â”‚
â”œâ”€â”€ sql/
â”‚ â”œâ”€â”€ ddl/
â”‚ â”‚ â”œâ”€â”€ create_staging_tables.sql
â”‚ â”‚ â”œâ”€â”€ create_production_tables.sql
â”‚ â”‚ â””â”€â”€ create_warehouse_tables.sql
â”‚ â”‚
â”‚ â””â”€â”€ queries/
â”‚ â”œâ”€â”€ analytical_queries.sql
â”‚ â”œâ”€â”€ data_quality_checks.sql
â”‚ â”œâ”€â”€ monitoring_queries.sql
â”‚ â”œâ”€â”€ load_dim_date.sql
â”‚ â””â”€â”€ load_fact_sales.sql
â”‚
â”œâ”€â”€ tests/
â”‚ â”œâ”€â”€ test_data_generation.py
â”‚ â”œâ”€â”€ test_ingestion.py
â”‚ â”œâ”€â”€ test_quality_checks.py
â”‚ â”œâ”€â”€ test_transformation.py
â”‚ â””â”€â”€ test_warehouse.py
â”‚
â”œâ”€â”€ docs/
â”‚ â”œâ”€â”€ architecture.md
â”‚ â””â”€â”€ dashboard_guide.md
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.sh
â”œâ”€â”€ analytics_summary.json
â”œâ”€â”€ pytest.ini
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```
---
## Code Organization & Maintainability

The project follows a modular pipeline-based structure where each folder represents a single responsibility.

### Scripts Structure
- `data_generation/` â€“ Synthetic data generation using Faker
- `ingestion/` â€“ Loads raw CSV data into staging schema
- `transformation/` â€“ Moves data from staging â†’ production â†’ warehouse
- `pipeline_orchestrator.py` â€“ Executes the end-to-end pipeline

This modular design makes the system easy to debug, test, and extend.

---

## Configuration Options
- Database credentials are managed using environment variables
- Configuration values are centralized in `config/config.yaml`
- Scripts can be executed independently or via the orchestrator
- Docker ensures a consistent runtime environment

---

## Troubleshooting Guide

| Issue | Solution |
|-----|---------|
| Dashboard not updating | Refresh Power BI or rerun analytics scripts |
| Duplicate records | Verify staging truncation logic |
| Slow queries | Use warehouse aggregate tables |
| Database connection error | Check Docker PostgreSQL container status |

---
## Architecture Decisions (Why?)
***Why PostgreSQL?***

Open-source, reliable RDBMS

Strong support for analytical queries

Schema-based separation (staging / production / warehouse)

Excellent integration with Python & BI tools

***Why Star Schema for Warehouse?***

Optimized for analytics & BI queries

Faster aggregations

Simple joins (fact â†’ dimensions)

Industry-standard dimensional modeling

***Why Layered Schemas?***

Staging â†’ raw ingestion (audit & recovery)

Production â†’ clean transactional truth

Warehouse â†’ analytics-optimized structure

---
## Technical Maintainability
Setup Instructions 
Prerequisites
```
Python 3.9+

Docker & Docker Compose

PostgreSQL

Power BI Desktop / Tableau Public
```
***Installation***
```
git clone <repo-url>
cd ecommerce-data-pipeline
pip install -r requirements.txt
docker-compose up -d
```
---
## Data Model Documentation 
Staging Schema
Exact CSV replica
Minimal validation
Temporary storage
Purpose: raw data audit & recovery
***Production Schema (3NF)***
Why 3NF?
Eliminates redundancy
Ensures data integrity
Supports transactional correctness
***Features:***
Primary & foreign keys
Cleaned data
Referential integrity enforced
Warehouse Schema (Star Schema)
Structure
Fact Table: fact_sales
Dimensions:
dim_customer
dim_product
dim_date
dim_payment
Aggregates: precomputed KPIs

***Why Star Schema?***
BI-friendly
Faster joins
Easier metrics calculation
SCD (Slowly Changing Dimensions)
Type 2 implemented
Maintains historical changes (e.g., customer segment changes)
Ensures accurate historical analytics
Index Strategy
Indexes on:
Foreign keys
Date keys
Frequently filtered columns
Improves dashboard performance

## Dashboard & Analytics Documentation 
Dashboard Pages Explained
Page 1: Executive Summary
Purpose: High-level business overview
Visuals:

Total Revenue

Total Transactions

Average Order Value

Profit Margin

Monthly Revenue Trend

Top Categories

Payment Method Distribution

Geographic Sales Map

Page 2: Product Analysis

Purpose: Product performance insights

Insights:

Top 10 products contribute majority revenue

Electronics category has highest profit margin

Long-tail products generate lower volume

Page 3: Customer Insights

Purpose: Customer behavior analysis

Insights:

VIP customers contribute major revenue share

Customer retention trends visible

CLV comparison across segments

Page 4: Geographic & Trends

Purpose: Location & time-based analysis

Insights:

Top 5 states generate most revenue

Weekend sales outperform weekdays

Seasonal trends identified

Metric Definitions (CRITICAL FOR SCORING)
Total Revenue
SUM(quantity Ã— unit_price âˆ’ discount)

Average Order Value (AOV)
Total Revenue / Total Orders

Profit Margin
(Total Revenue âˆ’ Cost) / Total Revenue Ã— 100

Customer Lifetime Value (CLV)
CLV = Average Order Value Ã— Purchase Frequency Ã— Customer Lifespan


âœ” Metric definitions explicitly documented (as required)

User Guidance (Dashboard Interaction)

Use Date Range filter to change analysis window

Click on categories/products to drill down

Cross-filtering enabled across visuals

Hover to view tooltips

Business Insights (Documented)

Revenue concentration in few products

High-value customers drive profitability

Geographic sales imbalance

Payment method preferences vary by region
ğŸ§¾ Declaration âœ… (THIS WAS MISSING EARLIER)

Declaration

I hereby declare that this project titled â€œPower BI E-Commerce Analytics Dashboardâ€ is an original work carried out by me.
The data, analysis, and dashboard visuals presented in this project are created for academic and learning purposes.
Any references or tools used have been duly acknowledged, and this work has not been submitted elsewhere for any other degree or certification.

Name: Lahari Sri
Course: B.Tech (CSE)
Institution: â€”â€”â€”
Date: â€”â€”â€”

ğŸ Conclusion

This Power BI dashboard effectively transforms raw e-commerce data into meaningful visual insights, enabling better business understanding and decision-making.
